{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive Pipeline Rerun (Fixed Splitter) + Chunk-Size Experiment\n",
    "\n",
    "After fixing the recursive splitter (sliding-window → standard RecursiveCharacterTextSplitter),\n",
    "this notebook reruns ALL recursive pipelines:\n",
    "\n",
    "- **4 Chapter 5 replacements** (dense/hybrid × rerank/no-rerank, all 1000/30)\n",
    "- **9 Chunk-Size Experiment** (500/30, 1000/30, 1500/50 × 3 pipeline types)\n",
    "- **3 overlap** → 10 unique configs total\n",
    "\n",
    "**Prerequisites:** Run `python prebuild_all_recursive.py` first to rebuild ALL BM25 indices.\n",
    "\n",
    "**Results:**\n",
    "- Chapter 5 results → `./results/Washington/final_run_42Q/` (overwrites old recursive CSVs)\n",
    "- Chunk-Size results → `./results/Washington/chunk_size_experiment/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nos.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"JAVA_HOME\"] = r\"C:\\Program Files\\Eclipse Adoptium\\jdk-17.0.18.8-hotspot\"\n\nimport torch  # MUST import before pandas to avoid DLL conflict on Windows\nimport gc\nimport pandas as pd\nfrom ragas import evaluate\nfrom datasets import Dataset\nfrom packages.person import Person\nfrom packages.globals import EMBEDDINGS\nfrom packages.llm_config import LLMConfig\nfrom packages.evaluate_rag import EvaluationPipeline\nfrom packages.vector_store_handler import VectorStoreHandler\nfrom packages.document_processing import DocumentProcessing\nfrom packages.bm25_retriever import BM25Retriever\nfrom packages.vector_store_handler import HybridRetriever\nfrom packages.init_chain import InitializeQuesionAnsweringChain\n\nfrom ragas.metrics import (\n    answer_relevancy,\n    faithfulness,\n    context_recall,\n    context_precision,\n    answer_correctness,\n    answer_similarity,\n)\n\n%load_ext autoreload\n%autoreload 2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_memory():\n",
    "    gc.collect()\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()\n",
    "    print(\"[INFO] Memory cleanup completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Washington\"\n",
    "RESULTS_CHAP5 = f\"./results/{name}/final_run_42Q\"\n",
    "RESULTS_CHUNK = f\"./results/{name}/chunk_size_experiment\"\n",
    "os.makedirs(RESULTS_CHAP5, exist_ok=True)\n",
    "os.makedirs(RESULTS_CHUNK, exist_ok=True)\n",
    "print(f\"[INFO] Chapter 5 results: {RESULTS_CHAP5}\")\n",
    "print(f\"[INFO] Chunk-Size results: {RESULTS_CHUNK}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config = LLMConfig(temperature=0.0)\n",
    "llm = llm_config.get_local_llm(use_openai=False)\n",
    "print(\"[INFO] LLM loaded once (Mistral 7B local) - will be reused for all pipelines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = pd.read_csv(f'./autogen_questions/{name}/hilfreich.csv')\n",
    "eval_questions = df_eval[\"question\"].tolist()\n",
    "question_types = df_eval[\"question_type\"].tolist()\n",
    "ground_truths = df_eval['ground_truth'].tolist()\n",
    "print(f\"[INFO] Loaded {len(eval_questions)} questions\")\n",
    "print(f\"[INFO] Question types: {pd.Series(question_types).value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_test(name: str, config: dict, questions: list[str], llm=None):\n",
    "    \"\"\"\n",
    "    Runs a single pipeline config. Supports both Chapter 5 (base BM25 path)\n",
    "    and chunk-size experiment (chunk_size-aware BM25 path).\n",
    "    \"\"\"\n",
    "    person = Person(name=name)\n",
    "    embedding = EMBEDDINGS\n",
    "    search_kwargs_num = 3\n",
    "\n",
    "    if llm is None:\n",
    "        llm_config = LLMConfig(temperature=0.0)\n",
    "        llm = llm_config.get_local_llm()\n",
    "\n",
    "    splitter_type = config[\"splitter_type\"]\n",
    "    chunk_size = config[\"chunk_size\"]\n",
    "    chunk_overlap = config[\"chunk_overlap\"]\n",
    "    retrieval_mode = config[\"retrieval_mode\"]\n",
    "    use_reranker = config[\"use_reranker\"]\n",
    "    bm25_path_mode = config.get(\"bm25_path_mode\", \"chunk_aware\")\n",
    "\n",
    "    vectorstore_handler = VectorStoreHandler(\n",
    "        embeddings=embedding,\n",
    "        search_kwargs_num=search_kwargs_num\n",
    "    )\n",
    "\n",
    "    database_path = vectorstore_handler._get_vector_store_path(\n",
    "        vector_store_name=person.name,\n",
    "        splitter_type=splitter_type,\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    print(f\"[INFO] FAISS path: {database_path}\")\n",
    "\n",
    "    if not os.path.exists(database_path):\n",
    "        print(f\"FAISS DB not found. Building...\")\n",
    "        doc_processor = DocumentProcessing(embeddings=embedding)\n",
    "        split_documents = doc_processor.get_chunked_documents(\n",
    "            directory_path=f\"./data/{person.name}\",\n",
    "            splitter_type=splitter_type,\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "        )\n",
    "        _, db = vectorstore_handler.create_db_and_retriever(\n",
    "            chunked_documents=split_documents,\n",
    "            vector_store_name=person.name,\n",
    "            splitter_type=splitter_type,\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Loading existing FAISS DB...\")\n",
    "        _, db = vectorstore_handler.get_db_and_retriever(\n",
    "            vector_store_name=person.name,\n",
    "            splitter_type=splitter_type,\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "        )\n",
    "\n",
    "    if db is None:\n",
    "        print(f\"[ERROR] Database creation/loading failed. Skipping.\")\n",
    "        return []\n",
    "\n",
    "    if retrieval_mode == 'hybrid':\n",
    "        # BM25 path depends on mode\n",
    "        if bm25_path_mode == \"base\":\n",
    "            bm25_index_dir = f\"./bm25_indexes/{name}_{splitter_type}\"\n",
    "        else:\n",
    "            bm25_index_dir = f\"./bm25_indexes/{name}_{splitter_type}_{chunk_size}_{chunk_overlap}\"\n",
    "        print(f\"[INFO] HYBRID Retriever | BM25: {bm25_index_dir} | exists: {os.path.isdir(bm25_index_dir)}\")\n",
    "        if not os.path.isdir(bm25_index_dir):\n",
    "            print(f\"[ERROR] BM25 index not found! Run prebuild_all_recursive.py first.\")\n",
    "            return []\n",
    "        bm25_retriever = BM25Retriever(bm25_index_dir)\n",
    "        retriever = HybridRetriever(db=db, bm25_retriever=bm25_retriever, k=search_kwargs_num)\n",
    "    else:\n",
    "        print(\"[INFO] DENSE Retriever\")\n",
    "        retriever = db.as_retriever(search_kwargs={\"k\": search_kwargs_num})\n",
    "\n",
    "    qa_chain = InitializeQuesionAnsweringChain(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        db=db,\n",
    "        person=person,\n",
    "        search_kwargs_num=search_kwargs_num,\n",
    "        use_reranker=use_reranker\n",
    "    )\n",
    "\n",
    "    eval_pipeline = EvaluationPipeline(\n",
    "        qa_chain=qa_chain,\n",
    "        eval_questions=questions\n",
    "    )\n",
    "\n",
    "    return eval_pipeline.generate_answers_with_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === COMPLETE RECURSIVE PIPELINE MATRIX ===\n",
    "# 10 unique configs: 4 Chapter 5 + 9 Chunk-Size (3 overlapping)\n",
    "#\n",
    "# save_to: where results go\n",
    "# bm25_path_mode: \"base\" = ./bm25_indexes/Washington_recursive\n",
    "#                 \"chunk_aware\" = ./bm25_indexes/Washington_recursive_1000_30\n",
    "\n",
    "evaluation_matrix = [\n",
    "    # ============================================================\n",
    "    # CHAPTER 5 REPLACEMENTS (use base BM25 path for compatibility)\n",
    "    # These overwrite the old sliding-window results in final_run_42Q\n",
    "    # ============================================================\n",
    "    {\"pipeline_name\": \"dense_recursive\",         \"splitter_type\": \"recursive\", \"chunk_size\": 1000, \"chunk_overlap\": 30, \"retrieval_mode\": \"dense\",  \"use_reranker\": False, \"save_to\": [\"chap5\"]},\n",
    "    {\"pipeline_name\": \"dense_recursive_rerank\",  \"splitter_type\": \"recursive\", \"chunk_size\": 1000, \"chunk_overlap\": 30, \"retrieval_mode\": \"dense\",  \"use_reranker\": True,  \"save_to\": [\"chap5\"]},\n",
    "    {\"pipeline_name\": \"hybrid_recursive\",        \"splitter_type\": \"recursive\", \"chunk_size\": 1000, \"chunk_overlap\": 30, \"retrieval_mode\": \"hybrid\", \"use_reranker\": False, \"save_to\": [\"chap5\"], \"bm25_path_mode\": \"base\"},\n",
    "    {\"pipeline_name\": \"hybrid_recursive_rerank\", \"splitter_type\": \"recursive\", \"chunk_size\": 1000, \"chunk_overlap\": 30, \"retrieval_mode\": \"hybrid\", \"use_reranker\": True,  \"save_to\": [\"chap5\"], \"bm25_path_mode\": \"base\"},\n",
    "\n",
    "    # ============================================================\n",
    "    # CHUNK-SIZE EXPERIMENT: 500 / 30\n",
    "    # ============================================================\n",
    "    {\"pipeline_name\": \"dense_recursive_500\",         \"splitter_type\": \"recursive\", \"chunk_size\": 500,  \"chunk_overlap\": 30, \"retrieval_mode\": \"dense\",  \"use_reranker\": False, \"save_to\": [\"chunk\"]},\n",
    "    {\"pipeline_name\": \"dense_recursive_rerank_500\",  \"splitter_type\": \"recursive\", \"chunk_size\": 500,  \"chunk_overlap\": 30, \"retrieval_mode\": \"dense\",  \"use_reranker\": True,  \"save_to\": [\"chunk\"]},\n",
    "    {\"pipeline_name\": \"hybrid_recursive_rerank_500\", \"splitter_type\": \"recursive\", \"chunk_size\": 500,  \"chunk_overlap\": 30, \"retrieval_mode\": \"hybrid\", \"use_reranker\": True,  \"save_to\": [\"chunk\"]},\n",
    "\n",
    "    # ============================================================\n",
    "    # CHUNK-SIZE EXPERIMENT: 1000 / 30 (overlap with Chapter 5)\n",
    "    # These are NEW runs with the fixed splitter, saved to chunk_size_experiment\n",
    "    # The Chapter 5 versions above use the base BM25 path;\n",
    "    # these use the chunk_aware BM25 path for the chunk-size comparison.\n",
    "    # Since the FAISS DB is the same (same chunk_size/overlap), answers will be\n",
    "    # identical for dense pipelines. For hybrid, BM25 index content is the same\n",
    "    # (same chunks), just different path — so results should also be identical.\n",
    "    # We re-run anyway for clean, self-contained chunk experiment CSVs.\n",
    "    # ============================================================\n",
    "    {\"pipeline_name\": \"dense_recursive_1000\",         \"splitter_type\": \"recursive\", \"chunk_size\": 1000, \"chunk_overlap\": 30, \"retrieval_mode\": \"dense\",  \"use_reranker\": False, \"save_to\": [\"chunk\"]},\n",
    "    {\"pipeline_name\": \"dense_recursive_rerank_1000\",  \"splitter_type\": \"recursive\", \"chunk_size\": 1000, \"chunk_overlap\": 30, \"retrieval_mode\": \"dense\",  \"use_reranker\": True,  \"save_to\": [\"chunk\"]},\n",
    "    {\"pipeline_name\": \"hybrid_recursive_rerank_1000\", \"splitter_type\": \"recursive\", \"chunk_size\": 1000, \"chunk_overlap\": 30, \"retrieval_mode\": \"hybrid\", \"use_reranker\": True,  \"save_to\": [\"chunk\"]},\n",
    "\n",
    "    # ============================================================\n",
    "    # CHUNK-SIZE EXPERIMENT: 1500 / 50\n",
    "    # ============================================================\n",
    "    {\"pipeline_name\": \"dense_recursive_1500\",         \"splitter_type\": \"recursive\", \"chunk_size\": 1500, \"chunk_overlap\": 50, \"retrieval_mode\": \"dense\",  \"use_reranker\": False, \"save_to\": [\"chunk\"]},\n",
    "    {\"pipeline_name\": \"dense_recursive_rerank_1500\",  \"splitter_type\": \"recursive\", \"chunk_size\": 1500, \"chunk_overlap\": 50, \"retrieval_mode\": \"dense\",  \"use_reranker\": True,  \"save_to\": [\"chunk\"]},\n",
    "    {\"pipeline_name\": \"hybrid_recursive_rerank_1500\", \"splitter_type\": \"recursive\", \"chunk_size\": 1500, \"chunk_overlap\": 50, \"retrieval_mode\": \"hybrid\", \"use_reranker\": True,  \"save_to\": [\"chunk\"]},\n",
    "]\n",
    "\n",
    "print(f\"[INFO] {len(evaluation_matrix)} pipeline configs to run\")\n",
    "for c in evaluation_matrix:\n",
    "    print(f\"  {c['pipeline_name']:<35} size={c['chunk_size']}, mode={c['retrieval_mode']}, rerank={c['use_reranker']}, save={c['save_to']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === GENERATE ANSWERS + RAGAS SCORE + SAVE (per pipeline, crash-safe) ===\nmetric_columns = [\n    'context_precision', 'faithfulness', 'answer_relevancy',\n    'context_recall', 'answer_correctness', 'semantic_similarity'\n]\n\ncompleted = []\nskipped = []\nfailed = []\n\nfor i, config in enumerate(evaluation_matrix):\n    pipeline_name = config[\"pipeline_name\"]\n    save_to = config[\"save_to\"]\n\n    # Check if already completed (crash recovery)\n    all_exist = True\n    for target in save_to:\n        if target == \"chap5\":\n            path = f\"{RESULTS_CHAP5}/{pipeline_name}_final_results.csv\"\n        else:\n            path = f\"{RESULTS_CHUNK}/{pipeline_name}_final_results.csv\"\n        if not os.path.exists(path):\n            all_exist = False\n    if all_exist:\n        print(f\"[SKIP] {pipeline_name} — CSV already exists, skipping.\")\n        skipped.append(pipeline_name)\n        continue\n\n    print(f\"\\n{'='*60}\")\n    print(f\"[{i+1}/{len(evaluation_matrix)}] PIPELINE: {pipeline_name}\")\n    print(f\"{'='*60}\")\n\n    try:\n        # --- Step 1: Generate answers (LOCAL, no internet needed) ---\n        print(f\"[STEP 1/3] Generating answers with Mistral 7B...\")\n        generated_answers = run_single_test(\n            name=name, config=config, questions=eval_questions, llm=llm,\n        )\n        if not generated_answers:\n            print(f\"[WARN] No answers returned for {pipeline_name}\")\n            failed.append(pipeline_name)\n            cleanup_memory()\n            continue\n\n        df_results = pd.DataFrame(generated_answers)\n        df_results[\"question_type\"] = question_types\n        df_results[\"chunk_size\"] = config[\"chunk_size\"]\n        df_results[\"chunk_overlap\"] = config[\"chunk_overlap\"]\n        print(f\"[OK] {len(df_results)} answers generated\")\n\n        # Save raw answers immediately (backup before RAGAS)\n        for target in save_to:\n            if target == \"chap5\":\n                backup_path = f\"{RESULTS_CHAP5}/{pipeline_name}_answers_raw.csv\"\n            else:\n                backup_path = f\"{RESULTS_CHUNK}/{pipeline_name}_answers_raw.csv\"\n            df_results.to_csv(backup_path, index=False)\n            print(f\"[BACKUP] Raw answers saved to {backup_path}\")\n\n        # --- Step 2: RAGAS scoring (NEEDS INTERNET for OpenAI judge) ---\n        print(f\"[STEP 2/3] Running RAGAS evaluation (needs internet)...\")\n        df_results['ground_truth'] = ground_truths\n        df_results['ground_truths'] = df_results['ground_truth'].apply(lambda x: [x])\n\n        ragas_dataset = Dataset.from_pandas(df_results)\n        ragas_results = evaluate(\n            ragas_dataset,\n            metrics=[\n                context_precision, faithfulness, answer_relevancy,\n                context_recall, answer_correctness, answer_similarity,\n            ]\n        )\n        df_ragas_scores = ragas_results.to_pandas()\n        df_scores_only = df_ragas_scores[metric_columns]\n\n        final_df = pd.concat([df_results.reset_index(drop=True), df_scores_only.reset_index(drop=True)], axis=1)\n        if 'ground_truth' in final_df.columns:\n            final_df = final_df.drop(columns=['ground_truth'])\n\n        # --- Step 3: Save final results ---\n        print(f\"[STEP 3/3] Saving final results...\")\n        for target in save_to:\n            if target == \"chap5\":\n                path = f\"{RESULTS_CHAP5}/{pipeline_name}_final_results.csv\"\n            else:\n                path = f\"{RESULTS_CHUNK}/{pipeline_name}_final_results.csv\"\n            final_df.to_csv(path, index=False)\n            print(f\"[SAVED] {path}\")\n\n        f_score = final_df['faithfulness'].mean()\n        ar_score = final_df['answer_relevancy'].mean()\n        cp_score = final_df['context_precision'].mean()\n        cr_score = final_df['context_recall'].mean()\n        print(f\"  F={f_score:.3f} AR={ar_score:.3f} CP={cp_score:.3f} CR={cr_score:.3f}\")\n        completed.append(pipeline_name)\n\n    except Exception as e:\n        print(f\"[ERROR] Pipeline {pipeline_name} failed: {e}\")\n        import traceback\n        traceback.print_exc()\n        failed.append(pipeline_name)\n\n    cleanup_memory()\n\nprint(f\"\\n{'='*60}\")\nprint(f\"DONE! Completed: {len(completed)}, Skipped: {len(skipped)}, Failed: {len(failed)}\")\nif failed:\n    print(f\"Failed pipelines: {failed}\")\nprint(f\"{'='*60}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === COMPARISON TABLE ===\n",
    "import glob\n",
    "\n",
    "print(\"\\n=== CHAPTER 5 RECURSIVE RESULTS (replaced) ===\")\n",
    "print(f\"{'Pipeline':<35} {'F':>8} {'AR':>8} {'CP':>8} {'CR':>8}\")\n",
    "print(\"-\" * 70)\n",
    "for csv_file in sorted(glob.glob(f\"{RESULTS_CHAP5}/*recursive*_final_results.csv\")):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    pname = os.path.basename(csv_file).replace('_final_results.csv', '')\n",
    "    print(f\"{pname:<35} {df['faithfulness'].mean():>8.3f} {df['answer_relevancy'].mean():>8.3f} {df['context_precision'].mean():>8.3f} {df['context_recall'].mean():>8.3f}\")\n",
    "\n",
    "print(f\"\\n=== CHUNK-SIZE EXPERIMENT RESULTS ===\")\n",
    "print(f\"{'Pipeline':<35} {'Size':>6} {'F':>8} {'AR':>8} {'CP':>8} {'CR':>8}\")\n",
    "print(\"-\" * 80)\n",
    "for csv_file in sorted(glob.glob(f\"{RESULTS_CHUNK}/*_final_results.csv\")):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    pname = os.path.basename(csv_file).replace('_final_results.csv', '')\n",
    "    cs = int(df['chunk_size'].iloc[0]) if 'chunk_size' in df.columns else '?'\n",
    "    print(f\"{pname:<35} {cs:>6} {df['faithfulness'].mean():>8.3f} {df['answer_relevancy'].mean():>8.3f} {df['context_precision'].mean():>8.3f} {df['context_recall'].mean():>8.3f}\")\n",
    "\n",
    "# Reference: dense_semantic (unaffected by splitter fix)\n",
    "ref_path = f\"{RESULTS_CHAP5}/dense_semantic_final_results.csv\"\n",
    "if os.path.exists(ref_path):\n",
    "    df_ref = pd.read_csv(ref_path)\n",
    "    print(f\"\\n{'dense_semantic (REF)':<35} {'---':>6} {df_ref['faithfulness'].mean():>8.3f} {df_ref['answer_relevancy'].mean():>8.3f} {df_ref['context_precision'].mean():>8.3f} {df_ref['context_recall'].mean():>8.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv311 (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
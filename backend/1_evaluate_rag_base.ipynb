{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"  # Entfernt Memory-Limit\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import pandas as pd\n",
    "import gc\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from ragas import evaluate\n",
    "from ragas.dataset_schema import RagasDataset\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from packages.person import Person\n",
    "from packages.globals import EMBEDDINGS\n",
    "from packages.llm_config import LLMConfig\n",
    "from packages.evaluate_rag import EvaluationPipeline\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from packages.vector_store_handler import VectorStoreHandler\n",
    "from packages.document_processing import DocumentProcessing\n",
    "from packages.bm25_retriever import BM25Retriever\n",
    "from packages.vector_store_handler import HybridRetriever\n",
    "from packages.init_chain import InitializeQuesionAnsweringChain\n",
    "\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    answer_correctness,\n",
    "    answer_similarity,\n",
    "    ResponseRelevancy\n",
    ")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_memory():\n",
    "    \"\"\"Gibt GPU/MPS Memory frei\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()\n",
    "    print(\"[INFO] Memory cleanup completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Washington\"\n",
    "use_full_dataset = True  # True f√ºr 42 Fragen, False f√ºr 7 Testfragen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config = LLMConfig(temperature=0.0)\n",
    "llm = llm_config.get_local_llm(use_openai=True)\n",
    "print(\"[INFO] LLM loaded once - will be reused for all pipelines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Daten laden\n",
    "df_eval = pd.read_csv(f'./autogen_questions/{name}/hilfreich.csv')\n",
    "\n",
    "if not use_full_dataset:\n",
    "    df_eval = df_eval.head(3)\n",
    "\n",
    "eval_questions = df_eval[\"question\"].tolist()\n",
    "question_types = df_eval[\"question_type\"].tolist()\n",
    "print(f\"[INFO] Loaded {len(eval_questions)} questions\")\n",
    "print(f\"[INFO] Question types: {pd.Series(question_types).value_counts().to_dict()}\")\n",
    "print(f\"[INFO] Columns in df_eval: {df_eval.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_test(name: str, config: dict, questions: list[str], llm=None):\n",
    "    \"\"\"\n",
    "    F√ºhrt einen einzelnen Testlauf f√ºr eine gegebene Konfiguration aus\n",
    "    und gibt die generierten Antworten mit Metadaten zur√ºck.\n",
    "    \"\"\"\n",
    "    \n",
    "    person = Person(name=name)\n",
    "    use_open_ai = False\n",
    "    if use_open_ai:\n",
    "        embedding = OpenAIEmbeddings()\n",
    "    else:\n",
    "        embedding = EMBEDDINGS\n",
    "    search_kwargs_num = 3\n",
    "    \n",
    "    if llm is None:\n",
    "        llm_config = LLMConfig(temperature=0.0)\n",
    "        llm = llm_config.get_local_llm()\n",
    "\n",
    "    splitter_type = config[\"splitter_type\"]\n",
    "    chunk_size = config[\"chunk_size\"]\n",
    "    chunk_overlap = config[\"chunk_overlap\"]\n",
    "    retrieval_mode = config[\"retrieval_mode\"]\n",
    "    use_reranker = config[\"use_reranker\"]\n",
    "    \n",
    "    vectorstore_handler = VectorStoreHandler(\n",
    "        embeddings=embedding,\n",
    "        search_kwargs_num=search_kwargs_num\n",
    "    )\n",
    "\n",
    "    database_path = vectorstore_handler._get_vector_store_path(\n",
    "        vector_store_name=person.name,\n",
    "        splitter_type=splitter_type,\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    print(f\"[INFO] FAISS path (expected): {database_path}\")\n",
    "\n",
    "    if not os.path.exists(database_path):\n",
    "        print(f\"Datenbank unter {database_path} nicht gefunden. Erstelle sie neu...\")\n",
    "        doc_processor = DocumentProcessing(embeddings=embedding)\n",
    "        split_documents = doc_processor.get_chunked_documents(\n",
    "            directory_path=f\"./data/{person.name}\",\n",
    "            splitter_type=splitter_type,\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "        )\n",
    "        _, db = vectorstore_handler.create_db_and_retriever(\n",
    "            chunked_documents=split_documents,\n",
    "            vector_store_name=person.name,\n",
    "            splitter_type=splitter_type,\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        print(f\"Lade existierende Datenbank von {database_path}...\")\n",
    "        _, db = vectorstore_handler.get_db_and_retriever(\n",
    "            vector_store_name=person.name,\n",
    "            splitter_type=splitter_type,\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "        )\n",
    "\n",
    "    if db is None:\n",
    "        print(f\"[ERROR] Database creation/loading failed for {splitter_type}. Skipping this configuration.\")\n",
    "        return []\n",
    "\n",
    "    if retrieval_mode == 'hybrid':\n",
    "        print(\"[INFO] Using HYBRID Retriever for this run.\")\n",
    "        bm25_index_dir = f\"./bm25_indexes/{name}_{config['splitter_type']}\"\n",
    "        print(f\"[INFO] Using BM25 index: {bm25_index_dir} | exists: {os.path.isdir(bm25_index_dir)}\")\n",
    "        bm25_retriever = BM25Retriever(bm25_index_dir)\n",
    "        retriever = HybridRetriever(db=db, bm25_retriever=bm25_retriever, k=search_kwargs_num)\n",
    "    else: # 'dense'\n",
    "        print(\"[INFO] Using DENSE Retriever for this run.\")\n",
    "        retriever = db.as_retriever(search_kwargs={\"k\": search_kwargs_num})\n",
    "\n",
    "    qa_chain = InitializeQuesionAnsweringChain(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        db=db,\n",
    "        person=person,\n",
    "        search_kwargs_num=search_kwargs_num,\n",
    "        use_reranker=use_reranker\n",
    "    )\n",
    "\n",
    "    eval_pipeline = EvaluationPipeline(\n",
    "        qa_chain=qa_chain,\n",
    "        eval_questions=questions\n",
    "    )\n",
    "\n",
    "    answer_list_with_metadata = eval_pipeline.generate_answers_with_metadata()\n",
    "    return answer_list_with_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_matrix = [\n",
    "    # === DENSE RETRIEVAL ===\n",
    "    {\"pipeline_name\": \"dense_recursive\",  \"splitter_type\": \"recursive\", \"chunk_size\": 1000, \"chunk_overlap\": 30, \"retrieval_mode\": \"dense\", \"use_reranker\": False},\n",
    "    {\"pipeline_name\": \"dense_sentence\",   \"splitter_type\": \"sentence_transformer\", \"chunk_size\": 256, \"chunk_overlap\": 30, \"retrieval_mode\": \"dense\", \"use_reranker\": False},\n",
    "    {\"pipeline_name\": \"dense_semantic\",   \"splitter_type\": \"semantic\", \"chunk_size\": 0, \"chunk_overlap\": 0, \"retrieval_mode\": \"dense\", \"use_reranker\": False},\n",
    "    \n",
    "    # === DENSE + RERANKER ===\n",
    "    {\"pipeline_name\": \"dense_recursive_rerank\", \"splitter_type\": \"recursive\", \"chunk_size\": 1000, \"chunk_overlap\": 30, \"retrieval_mode\": \"dense\", \"use_reranker\": True},\n",
    "    {\"pipeline_name\": \"dense_sentence_rerank\",  \"splitter_type\": \"sentence_transformer\", \"chunk_size\": 256, \"chunk_overlap\": 30, \"retrieval_mode\": \"dense\", \"use_reranker\": True},\n",
    "    {\"pipeline_name\": \"dense_semantic_rerank\",  \"splitter_type\": \"semantic\", \"chunk_size\": 0, \"chunk_overlap\": 0, \"retrieval_mode\": \"dense\", \"use_reranker\": True},\n",
    "    \n",
    "    # === HYBRID RETRIEVAL ===\n",
    "    {\"pipeline_name\": \"hybrid_recursive\", \"splitter_type\": \"recursive\", \"chunk_size\": 1000, \"chunk_overlap\": 30, \"retrieval_mode\": \"hybrid\", \"use_reranker\": False},\n",
    "    {\"pipeline_name\": \"hybrid_sentence\",  \"splitter_type\": \"sentence_transformer\", \"chunk_size\": 256, \"chunk_overlap\": 30, \"retrieval_mode\": \"hybrid\", \"use_reranker\": False},\n",
    "    {\"pipeline_name\": \"hybrid_semantic\",  \"splitter_type\": \"semantic\", \"chunk_size\": 0, \"chunk_overlap\": 0, \"retrieval_mode\": \"hybrid\", \"use_reranker\": False},\n",
    "    \n",
    "    # === HYBRID + RERANKER ===\n",
    "    {\"pipeline_name\": \"hybrid_recursive_rerank\", \"splitter_type\": \"recursive\", \"chunk_size\": 1000, \"chunk_overlap\": 30, \"retrieval_mode\": \"hybrid\", \"use_reranker\": True},\n",
    "    {\"pipeline_name\": \"hybrid_sentence_rerank\",  \"splitter_type\": \"sentence_transformer\", \"chunk_size\": 256, \"chunk_overlap\": 30, \"retrieval_mode\": \"hybrid\", \"use_reranker\": True},\n",
    "    {\"pipeline_name\": \"hybrid_semantic_rerank\",  \"splitter_type\": \"semantic\", \"chunk_size\": 0, \"chunk_overlap\": 0, \"retrieval_mode\": \"hybrid\", \"use_reranker\": True},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {}\n",
    "for config in evaluation_matrix:\n",
    "    pipeline_name = config[\"pipeline_name\"]\n",
    "    print(f\"\\n--- GENERATING ANSWERS FOR: {pipeline_name} ---\")\n",
    "    try:\n",
    "        generated_answers = run_single_test(\n",
    "            name=name,\n",
    "            config=config,\n",
    "            questions=eval_questions,\n",
    "            llm=llm,\n",
    "        )\n",
    "        df_results = pd.DataFrame(generated_answers)\n",
    "        df_results[\"question_type\"] = question_types\n",
    "        all_results[pipeline_name] = df_results\n",
    "        cleanup_memory()\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Pipeline {pipeline_name} failed: {e}\")\n",
    "        cleanup_memory()\n",
    "        continue\n",
    "print(f\"[INFO] Successfully completed {len(all_results)} pipelines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_columns = [\n",
    "        'context_precision',\n",
    "        'faithfulness',\n",
    "        'answer_relevancy',\n",
    "        'context_recall',\n",
    "        'answer_correctness',\n",
    "        'semantic_similarity'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truths = df_eval['ground_truth'].tolist()\n",
    "final_evaluation_results = {}\n",
    "\n",
    "# Loop through each test run you've completed\n",
    "for pipeline_name, df_generated in all_results.items():\n",
    "    print(f\"\\n{'='*20} RUNNING RAGAS ON: {pipeline_name} {'='*20}\")\n",
    "\n",
    "    # 1. Prepare the dataset for Ragas\n",
    "    # Add the ground truth and ensure it's in a list format\n",
    "    df_generated['ground_truth'] = ground_truths\n",
    "    df_generated['ground_truths'] = df_generated['ground_truth'].apply(lambda x: [x])\n",
    "\n",
    "    ragas_dataset = Dataset.from_pandas(df_generated)\n",
    "\n",
    "    # 2. Run the Ragas evaluation\n",
    "    ragas_results = evaluate(\n",
    "        ragas_dataset,\n",
    "        metrics=[\n",
    "            context_precision,\n",
    "            faithfulness,\n",
    "            answer_relevancy,\n",
    "            context_recall,\n",
    "            answer_correctness,\n",
    "            answer_similarity,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # 3. Combine data with the Ragas scores\n",
    "    df_ragas_scores = ragas_results.to_pandas()\n",
    "    df_scores_only = df_ragas_scores[metric_columns]\n",
    "\n",
    "    # Combine  original DataFrame with the new Ragas scores\n",
    "    final_df = pd.concat([df_generated.reset_index(drop=True), df_scores_only.reset_index(drop=True)], axis=1)\n",
    "    if 'ground_truth' in final_df.columns:\n",
    "        final_df = final_df.drop(columns=['ground_truth'])\n",
    "\n",
    "    # 4. Save the complete, final result\n",
    "    final_evaluation_results[pipeline_name] = final_df\n",
    "    os.makedirs(f\"./results/{name}\", exist_ok=True)\n",
    "    final_df.to_csv(f\"./results/{name}/{pipeline_name}_final_results.csv\", index=False)\n",
    "    print(f\"--- FINISHED: Final results for {pipeline_name} saved. ---\")\n",
    "    print(\"Final DataFrame columns:\", final_df.columns.tolist())\n",
    "\n",
    "print(f\"üéâ EVALUATION COMPLETE! Results saved for {len(final_evaluation_results)} pipelines.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
